version: '3.8'

services:
  # LiteLLM Proxy - The Gateway
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    restart: always # Auto-restart on crash or reboot
    ports:
      - "4000:4000"
    volumes:
      - ./config/lite-llm-config.yaml:/app/config.yaml
    command: [ "--config", "/app/config.yaml", "--detailed_debug" ]
    networks:
      - llm-network
    depends_on:
      - vllm
      - tei
    deploy:
      resources:
        limits:
          memory: 4G
      replicas: 2

  # Enterprise AI API - Custom logic and agent gateway
  api:
    build: .
    restart: always
    environment:
      - LITELLM_URL=http://litellm:4000
      - QDRANT_URL=http://qdrant:6333
      - JWT_SECRET_KEY=your-super-secret-key-change-me
    networks:
      - llm-network
    depends_on:
      - litellm
      - qdrant
    deploy:
      resources:
        limits:
          memory: 1G
      replicas: 2

  # Ollama - CPU-optimized LLM Inference Engine (GGUF Support)
  ollama:
    image: ollama/ollama:latest
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    networks:
      - llm-network
    deploy:
      resources:
        limits:
          memory: 8G # Sufficient for 7B models on CPU

  # TEI - Text Embeddings Inference (CPU Mode)
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-latest
    restart: always
    ports:
      - "8080:80"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./data/tei:/data
    networks:
      - llm-network
    deploy:
      resources:
        limits:
          memory: 2G
    command: --model-id BAAI/bge-m3 --port 80 --max-batch-tokens 16384 --max-concurrent-requests 512

  # Admin Dashboard - Streamlit UI for monitoring and management
  dashboard:
    build: .
    restart: always
    environment:
      - API_BASE_URL=http://api:8888
    networks:
      - llm-network
    depends_on:
      - api
    command: [ "streamlit", "run", "src/dashboard.py", "--server.port=8501", "--server.address=0.0.0.0" ]
    deploy:
      resources:
        limits:
          memory: 1G

  # Nginx - Reverse Proxy
  nginx:
    image: nginx:alpine
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - llm-network
    depends_on:
      - litellm
      - grafana
    deploy:
      resources:
        limits:
          memory: 256M

  # Qdrant - Vector Database (Production RAG)
  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    ports:
      - "6333:6333"
    volumes:
      - ./data/qdrant:/qdrant/storage
    networks:
      - llm-network
    deploy:
      resources:
        limits:
          memory: 4G

  # Redis - High-Performance Cache
  redis:
    image: redis:alpine
    restart: always
    volumes:
      - ./data/redis:/data
    networks:
      - llm-network
    deploy:
      resources:
        limits:
          memory: 1G

  # Postgres - Usage Tracking & Analytics
  postgres:
    image: postgres:15-alpine
    restart: always
    environment:
      - POSTGRES_USER=litellm
      - POSTGRES_PASSWORD=litellm_password
      - POSTGRES_DB=litellm_db
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - llm-network
    deploy:
      resources:
        limits:
          memory: 1G

  # Prometheus - Metrics Collection
  prometheus:
    image: prom/prometheus:latest
    restart: always
    volumes:
      - ./config/prometheus.yaml:/etc/prometheus/prometheus.yml
      - ./data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - "9090:9090"
    networks:
      - llm-network
    deploy:
      resources:
        limits:
          memory: 1G

  # Grafana - Metrics Dashboard
  grafana:
    image: grafana/grafana:latest
    restart: always
    ports:
      - "3000:3000"
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards/json
    networks:
      - llm-network
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          memory: 1G

  # Loki - Log Aggregation
  loki:
    image: grafana/loki:latest
    restart: always
    ports:
      - "3100:3100"
    volumes:
      - ./config/loki/loki-config.yaml:/etc/loki/local-config.yaml
      - ./data/loki:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - llm-network
    deploy:
      resources:
        limits:
          memory: 512M

  # Promtail - Log Shipping
  promtail:
    image: grafana/promtail:latest
    restart: always
    volumes:
      - ./config/promtail/promtail-config.yaml:/etc/promtail/config.yaml
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yaml
    networks:
      - llm-network
    depends_on:
      - loki
    deploy:
      resources:
        limits:
          memory: 256M

networks:
  llm-network:
    driver: bridge
