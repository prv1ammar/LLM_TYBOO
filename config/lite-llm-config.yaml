model_list:
  - model_name: internal-llm
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://ollama:11434
      api_key: "not-needed"

  - model_name: internal-embedding
    litellm_params:
      model: openai/BAAI/bge-m3
      api_base: http://tei:80/v1
      api_key: "not-needed"

  # Hybrid Cloud Fallback (Example using Groq for fast 70B inference)
  - model_name: high-reasoning-llm
    litellm_params:
      model: groq/deepseek-r1-distill-llama-70b
      api_key: os.environ/GROQ_API_KEY

general_settings:
  master_key: your_key_here # Replace with a secure key
  store_model_usage: true
  database_url: "postgresql://litellm:litellm_password@postgres:5432/litellm_db"

litellm_settings:
  callbacks: ['prometheus']
  success_callback: ["database"] # Track every successful request in Postgres
  failure_callback: ["database"]

cache:
  type: redis
  host: redis
  port: 6379
